{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlLRCdW/Z8ZnF+F3crt5bb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/celinaLind/pentagram/blob/main/Image_Generation_Walkthrough_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install modal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9To0XuWtaqc",
        "outputId": "6e9763f6-bd35-4b8c-8303-9586e7fb2a86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modal\n",
            "  Downloading modal-0.68.44-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from modal) (3.11.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from modal) (2024.12.14)\n",
            "Requirement already satisfied: click>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from modal) (8.1.7)\n",
            "Collecting fastapi (from modal)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting grpclib==0.4.7 (from modal)\n",
            "  Downloading grpclib-0.4.7.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf!=4.24.0,<6.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from modal) (4.25.5)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from modal) (13.9.4)\n",
            "Collecting synchronicity~=0.9.8 (from modal)\n",
            "  Downloading synchronicity-0.9.8-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from modal) (0.10.2)\n",
            "Requirement already satisfied: typer>=0.9 in /usr/local/lib/python3.10/dist-packages (from modal) (0.15.1)\n",
            "Collecting types-certifi (from modal)\n",
            "  Downloading types_certifi-2021.10.8.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting types-toml (from modal)\n",
            "  Downloading types_toml-0.10.8.20240310-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting watchfiles (from modal)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.6 in /usr/local/lib/python3.10/dist-packages (from modal) (4.12.2)\n",
            "Collecting h2<5,>=3.1.0 (from grpclib==0.4.7->modal)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: multidict in /usr/local/lib/python3.10/dist-packages (from grpclib==0.4.7->modal) (6.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->modal) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->modal) (2.18.0)\n",
            "Collecting sigtools>=4.0.1 (from synchronicity~=0.9.8->modal)\n",
            "  Downloading sigtools-4.0.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9->modal) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (1.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (1.18.3)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->modal)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi->modal) (2.10.3)\n",
            "Requirement already satisfied: anyio>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from watchfiles->modal) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.0.0->watchfiles->modal) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.0.0->watchfiles->modal) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio>=3.0.0->watchfiles->modal) (1.2.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3.1.0->grpclib==0.4.7->modal)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3.1.0->grpclib==0.4.7->modal)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->modal) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->modal) (2.27.1)\n",
            "Downloading modal-0.68.44-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading synchronicity-0.9.8-py3-none-any.whl (34 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_certifi-2021.10.8.3-py3-none-any.whl (2.1 kB)\n",
            "Downloading types_toml-0.10.8.20240310-py3-none-any.whl (4.8 kB)\n",
            "Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sigtools-4.0.1-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: grpclib\n",
            "  Building wheel for grpclib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpclib: filename=grpclib-0.4.7-py3-none-any.whl size=76218 sha256=df6ff2383e3ed665f26d7066b895a14ed55aa7eabee308cbad9208ce42e05297\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/c0/1c/3d807409d0c67efeab2949832ba409205b1b6fe03f739ae4c1\n",
            "Successfully built grpclib\n",
            "Installing collected packages: types-certifi, types-toml, sigtools, hyperframe, hpack, watchfiles, synchronicity, starlette, h2, grpclib, fastapi, modal\n",
            "Successfully installed fastapi-0.115.6 grpclib-0.4.7 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 modal-0.68.44 sigtools-4.0.1 starlette-0.41.3 synchronicity-0.9.8 types-certifi-2021.10.8.3 types-toml-0.10.8.20240310 watchfiles-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pejktuiLth42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modal\n",
        "\n",
        "Modal is a serverless container\n",
        "meaning it spins up and down the servers in use dependent on\n",
        "\n",
        "by default model containers spin down after 60 seconds\n",
        "- you can set directly @app.function(container_idle_timeout=*timeInSeconds*)\n",
        "- or always have a few servers running\n",
        "@app.function(keep_warm=*#ofServersToKeepActive*)\n",
        "  - this could cause issues with cost\n",
        "- Cron job or schedule keep_warm function where is runs after a set amount of time"
      ],
      "metadata": {
        "id": "bBb2gcJI1sIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modal.py\n",
        "\n",
        "import modal\n",
        "import io\n",
        "from fastapi import Response, HTTPException, Query, Request\n",
        "from datetime import datetime, timezone\n",
        "import os\n",
        "\n",
        "# modal endpoints is a fastAPI server under the hood\n",
        "# os allows access to environment variables\n",
        "\n",
        "# download model\n",
        "def download_model():\n",
        "  # why are the imports done within the function?\n",
        "  from diffusers import AutoPipelineForText2Image\n",
        "  import torch\n",
        "  # load model and set up pipeline\n",
        "  pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\",\n",
        "                                                   torch_dtype=torch.float16,\n",
        "                                                   # the lower the precision the worse the quality\n",
        "                                                   variant=\"fp16\")\n",
        "# set up image (i.e. docker container)\n",
        "image = (modal.Image.debian_slim()\n",
        "        # the below packages are quite robust so in order to save your local space\n",
        "         # we install it to the modal docker container\n",
        "            .pip_install(\"fastapi[standard]\", \"transformers\", \"accelerate\", \"diffusers\", \"requests\")\n",
        "            .run_function(download_model))\n",
        "\n",
        "app = modal.App(\"sd-demo\", image=image)\n",
        "\n",
        "#what is cls??\n",
        "@app.cls(\n",
        "    image=image,\n",
        "    gpu=\"A10G\", #can distribute the load across multiple GPUs if you receive large loads\n",
        "    # pick GPU based on case, the above is the cheapest quality gpu from Modal\n",
        "    container_idle_timeout=300,\n",
        "    secrets=[modal.Secret.from_name(\"API_KEY_NAME\")]\n",
        ")\n",
        "class Model:\n",
        "  @modal.build() # decorator for methods that should execute at build time\n",
        "  @modal.enter() # decorator for methods that needs to be called when new container is started\n",
        "  # method called on app build\n",
        "  def load_weights(self):\n",
        "    from diffusers import AutoPipelineForText2Image\n",
        "    import torch\n",
        "\n",
        "    self.pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\",\n",
        "                                          torch_dtype=torch.float16,\n",
        "                                          variant=\"fp16\")\n",
        "    self.pipe.to(\"cuda\") # cuda is NVIDIA software stack (cuda kernels??)\n",
        "    # ^ this tells modal to make sure it runs on the GPU\n",
        "\n",
        "    self.API_KEY_NAME = os.environ[\"API_KEY_NAME\"]\n",
        "\n",
        "  # create modal endpoint (api call)\n",
        "  @model.web_endpoint()\n",
        "  # a query parameter is like a youtube video id\n",
        "  def generate(self, request: Request, prompt: str = Query(..., description=\"The prompt for image generation\")):\n",
        "    # from request headers get the API key\n",
        "    api_key = request.headers.get(\"X-API-KEY\") #the api key 'X-API-KEY' might have a different name\n",
        "    if api_key != self.API_KEY_NAME:\n",
        "      # don't allow use of api and throw error code\n",
        "      raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
        "\n",
        "    # if you increase inference steps: Quality better, latency worse\n",
        "    # test with different inference step amounts {10 or 20} <== will take a lot longer to deploy/generate\n",
        "    # guidance_scale: is how well the model conforms/confines to the prompt\n",
        "    # before .images[0] it returns a list of string images\n",
        "    image = self.pipe(prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
        "\n",
        "    # create a buffer or in memory file to save image\n",
        "    buffer = io.BytesIO()\n",
        "\n",
        "    # save the image to buffer\n",
        "    image.save(buffer, format=\"JPEG\")\n",
        "\n",
        "    # return image from buffer\n",
        "    # media_type: lets the browser know what it will be getting\n",
        "    return Response(content=buffer.getvalue(), media_type=\"image/jpeg\")\n",
        "\n",
        "    #need to incorporate validation to confirm correct image type is being returned\n",
        "  # the initial deployment/run it has to download the model meaning it could take 5-10 minutes or more\n",
        "  # the end result of the deployment is a URL generated by Modal\n",
        "\n",
        "  # after we get the URL we need to SECURE the URL so random ppl can't access the API\n",
        "  # can be done using an API key (can add a secret directly in Modal using python-secrets library to generate a secure secret token)\n",
        "\n",
        "  @modal.web_endpoint()\n",
        "  def health(self): # not secure but doesn't have computations so not a big deal\n",
        "    \"\"\"Lightweght endpoint for keeping the container warm\"\"\"\n",
        "    return {\"status\": \"healthy\", \"timestamp\": datetime.now(timezone.utc).isoformat()}\n",
        "\n",
        "  #warm-keeping fcn that runs every 5 minutes\n",
        "  @app.function(\n",
        "      schedule=modal.Cron(\"*/5 * * * *\"),\n",
        "      secrets=[modal.Secret.from_name(\"API_KEY_NAME\")]\n",
        "  )\n",
        "  # call name of function in URL\n",
        "  def keep_warm():\n",
        "    health_url: \"vercelURL-functionName.modal.run\"\n",
        "    generate_url: \"vercelurl-functionName.modal.run\"\n",
        "\n",
        "    # first check health endpt (no API key needed)\n",
        ""
      ],
      "metadata": {
        "id": "SdWLQc9PtW-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in another repo to call api from backend\n",
        "# in route.ts file\n",
        "\n",
        "# keep in mind: NEVER call external API on frontend always call on backend\n",
        "\n",
        "# first get prompt from user ==> get from request body\n",
        "\n",
        "# save modal url as environment variable\n",
        "const url = new URL(\"construct new url using deployed modal url\")\n",
        "\n",
        "url.searchParams.set(\"prompt\", text)\n",
        "\n",
        "# after url created with parameters\n",
        "# create fetch request\n",
        "const response = await fetch(url.toString(), {\n",
        "    method: \"GET\",\n",
        "    headers: {\n",
        "        \"X-API-KEY\": process.env.API_KEY_NAME,\n",
        "        Accept: \"image/jpeg\"\n",
        "    }\n",
        "})\n",
        "\n",
        "#error handling\n",
        "\n",
        "\n",
        "#Add blob storage for generated images\n",
        "# create database in vercel > blob > will provide you blob read/write token to use\n",
        "# install and import vercel/blob and corresponding put functions\n",
        "# import crypto\n",
        "\n",
        "# to read data received from API\n",
        "const imageBuffer = await response.arrayBuffer();\n",
        "\n",
        "# using the crypto library to create unique token names for each generated image\n",
        "const filename = `${crypto.randomUUID()}.jpg`\n",
        "\n",
        "# store filepath you get from vercel into the database\n",
        "\n",
        "const blob = await put(filename, imageBuffer,\n",
        "                        { access: \"public\",\n",
        "                       contentType: \"image/jpeg\"})\n",
        "# when you add it to vercel, vercel returns a public URL for use\n",
        "# then return the blob image\n",
        "return NextResponse.json({\n",
        "    ...\n",
        "})"
      ],
      "metadata": {
        "id": "kv_dM7So3oGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handle frontend response\n",
        "\n",
        "const [imageUrl, setImageUrl] = useState...\n",
        "\n",
        "# if data imageUrl doesn't show return error\n",
        "# if successful update state variable on load and create new image\n",
        "# then show generated image on frontend"
      ],
      "metadata": {
        "id": "gvh1Ehui7DZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Securing App\n",
        "\n",
        "## Public api endpoint\n",
        "-- need to secure the generate-image endpoint\n",
        "\n",
        "**Things to do** (refactor code):\n",
        "1. send request through server\n",
        "2. use server-actions\n",
        "\n",
        "separate client side code from what is being run on server by creating a separate server-action within actions folder\n",
        "\n",
        "\"use server\" < put at the top of the file\n",
        "-- since it is running on the server you can use the secret since no one can see the headers you don't want to see it\n",
        "\n",
        "### make component for client side code (if possible)\n",
        "--> instead of making fetch request in \"use client\" file you call a function within a \"use server\" file that handles the API request and response\n",
        "\n",
        "\n",
        "* will need to verify the api secret in the route file\n",
        "\n",
        "## Other securing features:\n",
        "- encryption\n",
        "- house modal URL in the env file"
      ],
      "metadata": {
        "id": "9ym9Fi_P8aJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Postgres\n",
        "can be done through Vercel\n",
        "- store user data\n",
        "  - image url\n",
        "  - user prompt\n",
        "  - creation time\n",
        "  - latency for generation (const now = datetime.now())\n",
        "    - can be used for analytics to log trends, cross-referencing with model dashboard, etc.\n",
        "  \n",
        "--> could technically provide multiple images since they are provided in list format\n",
        "\n",
        "--> create system to deter inappropriate or malicious prompt image generation\n",
        "  - can be done through LLM directly or within prompts"
      ],
      "metadata": {
        "id": "8NI_eQLFCK3V"
      }
    }
  ]
}